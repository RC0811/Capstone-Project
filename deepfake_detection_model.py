# -*- coding: utf-8 -*-
"""Deepfake detection Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12fhWKtimelhIbYd0TUSYJ6fwGA5zgrwp

Part-1-Setup
"""

!pip -q install -U transformers==4.43.3 timm==1.0.9 opencv-python-headless==4.10.0.84 \
                      librosa==0.10.1 soundfile==0.12.1 mediapipe==0.10.14 ffmpeg-python==0.2.0 \
                      scikit-learn==1.5.1 tqdm==4.66.4

try:
    !pip -q install "git+https://github.com/m-bain/whisperX.git"
    HAS_WHISPERX = True
except Exception:
    HAS_WHISPERX = False

import os, torch, random, numpy as np
print("Torch:", torch.__version__, "| CUDA:", torch.cuda.is_available())

DATA_ROOT = "/content/videos"
WORK_DIR  = "/content/df_pipeline"
os.makedirs(DATA_ROOT, exist_ok=True)
os.makedirs(WORK_DIR, exist_ok=True)

def seed_all(seed=42):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
seed_all(42)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

"""Video Upload"""

from google.colab import files
import shutil, os

for split in ["real", "fake"]:
    os.makedirs(f"{DATA_ROOT}/{split}", exist_ok=True)
    print(f"Select {split.upper()} videos (multi-select allowed)â€¦")
    up = files.upload()
    for fn in up.keys():
        shutil.move(fn, f"{DATA_ROOT}/{split}/{fn}")

print("Uploaded:")
!find /content/videos -maxdepth 2 -type f -printf '%P\n' | sed -n '1,50p'

"""Part-2-Audio/Visual Preprocessing"""

import os, cv2, numpy as np, librosa, ffmpeg, timm, mediapipe as mp
import torchvision.transforms as T
from tqdm import tqdm
from transformers import Wav2Vec2Processor, Wav2Vec2Model

wav_proc  = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
wav_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h").to(DEVICE).eval()
vis_backbone = timm.create_model("resnet18", pretrained=True, num_classes=0).to(DEVICE).eval()

AUDIO_CACHE = os.path.join(WORK_DIR, "audio_cache")
os.makedirs(AUDIO_CACHE, exist_ok=True)

def _probe_audio_stream(video_path):
    try:
        info = ffmpeg.probe(video_path)
        streams = info.get("streams", [])
        for i, s in enumerate(streams):
            if s.get("codec_type") == "audio" and int(s.get("channels", 0)) > 0:
                return True, s.get("index", i)
    except Exception:
        pass
    return False, None

def _extract_wav_ffmpeg(video_path, sr=16000):
    base = os.path.splitext(os.path.basename(video_path))[0]
    out_wav = os.path.join(AUDIO_CACHE, f"{base}_{sr}Hz_mono.wav")
    if os.path.exists(out_wav): return out_wav
    has_audio, a_idx = _probe_audio_stream(video_path)
    if not has_audio: raise ValueError("No audio stream")
    try:
        (
            ffmpeg.input(video_path)
            .output(out_wav, ac=1, ar=sr, acodec="pcm_s16le", format="wav", loglevel="error")
            .overwrite_output().run()
        )
    except ffmpeg.Error:
        (
            ffmpeg.input(video_path)
            .output(out_wav, **{"map": f"0:{a_idx}"}, ac=1, ar=sr, acodec="pcm_s16le", format="wav", loglevel="error")
            .overwrite_output().run()
        )
    return out_wav

def read_audio_16k(video_path):
    wav_path = _extract_wav_ffmpeg(video_path, sr=16000)
    y, _ = librosa.load(wav_path, sr=16000, mono=True)
    if not isinstance(y, np.ndarray) or y.size == 0:
        raise ValueError("Empty audio after extraction")
    return y

def wav2vec_feats(y16k):
    ivals = wav_proc(y16k, sampling_rate=16000, return_tensors="pt").input_values.to(DEVICE)
    with torch.no_grad():
        hs = wav_model(ivals).last_hidden_state.squeeze(0)
    return hs.cpu().numpy()

def iter_frames(path, resize=(224,224)):
    cap = cv2.VideoCapture(path)
    if not cap.isOpened(): return []
    frames = []
    while True:
        ok, fr = cap.read()
        if not ok: break
        fr = cv2.cvtColor(cv2.resize(fr, resize), cv2.COLOR_BGR2RGB)
        frames.append(fr)
    cap.release()
    return frames

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)
LIP_IDX = list(range(61,81)) + list(range(81,88))
def crop_mouth(frame_rgb):
    h, w, _ = frame_rgb.shape
    res = face_mesh.process(frame_rgb)
    if not res.multi_face_landmarks:
        s = min(h,w)//2; y0=(h-s)//2; x0=(w-s)//2
        return cv2.resize(frame_rgb[y0:y0+s, x0:x0+s], (224,224))
    lm = res.multi_face_landmarks[0].landmark
    xs, ys = [], []
    for i in LIP_IDX:
        xi, yi = int(lm[i].x*w), int(lm[i].y*h)
        if 0<=xi<w and 0<=yi<h: xs.append(xi); ys.append(yi)
    if not xs or not ys:
        s = min(h,w)//2; y0=(h-s)//2; x0=(w-s)//2
        return cv2.resize(frame_rgb[y0:y0+s, x0:x0+s], (224,224))
    x0,x1 = max(min(xs)-10,0), min(max(xs)+10,w)
    y0,y1 = max(min(ys)-10,0), min(max(ys)+10,h)
    roi = frame_rgb[y0:y1, x0:x1]
    if roi.size==0:
        s = min(h,w)//2; y0=(h-s)//2; x0=(w-s)//2
        roi = frame_rgb[y0:y0+s, x0:x0+s]
    return cv2.resize(roi, (224,224))

to_tensor = T.Compose([
    T.ToTensor(),
    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

def vis_feats_from_frames(frames):
    if not frames: return np.zeros((1,512), np.float32)
    feats = []
    with torch.no_grad():
        for fr in frames:
            x = to_tensor(crop_mouth(fr)).unsqueeze(0).to(DEVICE)
            v = vis_backbone(x).squeeze(0).cpu().numpy()
            feats.append(v)
    return np.stack(feats, axis=0)

def linear_align(a_seq, v_seq):
    Ta, Tv = len(a_seq), len(v_seq)
    if Tv==0 or Ta==0: return a_seq[:0], v_seq[:0]
    idx = (np.linspace(0, Ta-1, Tv)).round().astype(int)
    return a_seq[idx], v_seq

PROC_DIR = os.path.join(WORK_DIR, "proc")
os.makedirs(PROC_DIR, exist_ok=True)

def process_one(video_path, label):
    base = os.path.splitext(os.path.basename(video_path))[0]
    out = os.path.join(PROC_DIR, f"{base}.npz")
    if os.path.exists(out): return out
    y = read_audio_16k(video_path)
    A = wav2vec_feats(y)
    frames = iter_frames(video_path, (224,224))
    V = vis_feats_from_frames(frames)
    a_al, v_al = linear_align(A, V)
    np.savez_compressed(out, audio=a_al, visual=v_al, label=np.array([label], np.int64))
    return out

def process_all(data_root=DATA_ROOT):
    index = []
    for cls, ylab in [("real",0),("fake",1)]:
        folder = f"{data_root}/{cls}"
        vids = [f for f in os.listdir(folder) if f.lower().endswith((".mp4",".mov",".mkv",".avi",".webm"))]
        print(f"{cls} videos:", len(vids))
        for f in tqdm(vids):
            vp = os.path.join(folder,f)
            try:
                p = process_one(vp, ylab)
                index.append((p, ylab))
            except Exception as e:
                print(f"[Skip] {f} -> {e}")
    return index

INDEX = process_all()
print("Processed:", len(INDEX), "samples")

"""Part-3-WhisperX"""

import numpy as np, torch, cv2
WHISPERX_ACTIVE = False
if 'HAS_WHISPERX' in globals() and HAS_WHISPERX:
    try:
        import whisperx
        asr_model = whisperx.load_model("small", device=DEVICE)
        align_model, metadata = whisperx.load_align_model(language_code="en", device=DEVICE)
        WHISPERX_ACTIVE = True
        print("WhisperX ready.")
    except Exception as e:
        print("WhisperX not active:", e)

def _get_fps(path, default=25.0):
    cap = cv2.VideoCapture(path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 0.0
    cap.release()
    return float(fps) if fps and fps>1e-3 else float(default)

_old_process_one = process_one
def process_one(video_path, label):
    base = os.path.splitext(os.path.basename(video_path))[0]
    out = os.path.join(PROC_DIR, f"{base}.npz")
    if os.path.exists(out): return out

    y = read_audio_16k(video_path)
    A = wav2vec_feats(y)

    frames = iter_frames(video_path, (224,224))
    V = vis_feats_from_frames(frames)
    Tv = len(V)

    if WHISPERX_ACTIVE and Tv>0 and len(A)>0:
        try:
            audio_t = torch.tensor(y).float().to(DEVICE)
            with torch.no_grad():
                seg = asr_model.transcribe(audio_t, batch_size=8)
                ali = whisperx.align(seg["segments"], align_model, metadata, audio_t, 16000, DEVICE)
            dur = float(ali["word_segments"][-1]["end"]) if ali.get("word_segments") else len(y)/16000.0
            fps = _get_fps(video_path)
            t_v = np.arange(Tv, dtype=np.float32) / max(fps,1e-6)
            t_v = np.clip(t_v, 0.0, max(dur,1e-6))
            Ta = len(A)
            idx = (t_v / max(dur,1e-6) * max(Ta-1,0)).round().astype(int)
            a_al = A[np.clip(idx,0,max(Ta-1,0))]
            v_al = V
        except Exception:
            a_al, v_al = linear_align(A, V)
    else:
        a_al, v_al = linear_align(A, V)

    np.savez_compressed(out, audio=a_al, visual=v_al, label=np.array([label], np.int64))
    return out

if WHISPERX_ACTIVE:
    INDEX = process_all()
    print("Processed (WhisperX):", len(INDEX))

"""Part-4-Dataset/Dataloading"""

import numpy as np, torch
from torch.utils.data import Dataset, DataLoader, random_split

class AVSeqNPZ(Dataset):
    def __init__(self, items): self.items = items
    def __len__(self): return len(self.items)
    def __getitem__(self, i):
        path, label = self.items[i]
        d = np.load(path)
        A = d["audio"].astype(np.float32)
        V = d["visual"].astype(np.float32)
        X = np.concatenate([A, V], axis=1)
        return torch.from_numpy(X), torch.tensor(int(label), dtype=torch.long)

def pad_collate(batch):
    xs, ys = zip(*batch)
    lens = [x.size(0) for x in xs]
    Tm = max(lens)
    Xp = torch.zeros(len(xs), Tm, xs[0].size(1))
    for i,x in enumerate(xs): Xp[i,:x.size(0)] = x
    return Xp, torch.tensor(ys), torch.tensor(lens)

full = AVSeqNPZ(INDEX)
n_train = max(1, int(0.8*len(full))); n_val = max(1, len(full)-n_train)
train_ds, val_ds = random_split(full, [n_train, n_val], generator=torch.Generator().manual_seed(42))
train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,  collate_fn=pad_collate)
val_loader   = DataLoader(val_ds,   batch_size=2, shuffle=False, collate_fn=pad_collate)
print("Train/Val sizes:", len(train_ds), len(val_ds))

"""Part-5-Model(BiLSTM and mean pool)"""

import torch.nn as nn, torch

class AVBiLSTM(nn.Module):
    def __init__(self, in_dim=1280, hid=256, n_layers=1, num_classes=2):
        super().__init__()
        self.rnn = nn.LSTM(in_dim, hid, num_layers=n_layers, bidirectional=True, batch_first=True)
        self.head = nn.Sequential(nn.LayerNorm(hid*2), nn.Linear(hid*2, num_classes))
    def forward(self, x, lengths):
        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)
        H, _ = self.rnn(packed)
        H, _ = nn.utils.rnn.pad_packed_sequence(H, batch_first=True)
        B,T,D = H.size()
        mask = (torch.arange(T, device=H.device).unsqueeze(0) < lengths.unsqueeze(1)).float().unsqueeze(-1)  # [B,T,1]
        H_sum = (H * mask).sum(dim=1) / mask.sum(dim=1).clamp_min(1.0)
        return self.head(H_sum)

model = AVBiLSTM().to(DEVICE)
print("Params (M):", round(sum(p.numel() for p in model.parameters())/1e6, 3))

"""Part-6-Training and validating"""

from sklearn.metrics import accuracy_score, f1_score
import numpy as np, torch, torch.nn as nn

EPOCHS = 8
opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)
crit = nn.CrossEntropyLoss()

hist = {"tr_loss":[], "tr_acc":[], "tr_f1":[],
        "va_loss":[], "va_acc":[], "va_f1":[]}

def run_epoch(loader, train=True):
    model.train(train)
    losses, y_true, y_pred = [], [], []
    for X, Y, L in loader:
        X, Y, L = X.to(DEVICE), Y.to(DEVICE), L.to(DEVICE)
        with torch.set_grad_enabled(train):
            logits = model(X, L); loss = crit(logits, Y)
        if train:
            opt.zero_grad(); loss.backward(); opt.step()
        losses.append(loss.item())
        y_true.extend(Y.tolist()); y_pred.extend(logits.argmax(1).tolist())
    return np.mean(losses), accuracy_score(y_true,y_pred), f1_score(y_true,y_pred, average="weighted")

best_f1, best_state = -1, None
for ep in range(1, EPOCHS+1):
    tr = run_epoch(train_loader, True)
    va = run_epoch(val_loader,   False)
    hist["tr_loss"].append(tr[0]); hist["tr_acc"].append(tr[1]); hist["tr_f1"].append(tr[2])
    hist["va_loss"].append(va[0]); hist["va_acc"].append(va[1]); hist["va_f1"].append(va[2])
    print(f"Epoch {ep:02d} | Train loss {tr[0]:.3f} acc {tr[1]:.3f} f1 {tr[2]:.3f} || Val loss {va[0]:.3f} acc {va[1]:.3f} f1 {va[2]:.3f}")
    if va[2] > best_f1:
        best_f1 = va[2]; best_state = {k:v.cpu() for k,v in model.state_dict().items()}

MODEL_PATH = f"{WORK_DIR}/bilstm_attn.pth"
if best_state is not None:
    torch.save(best_state, MODEL_PATH)
    print("Saved best:", MODEL_PATH, "| best F1:", round(best_f1,4))
else:
    print("No validation improvement captured.")

"""Part-7-training curves"""

import matplotlib.pyplot as plt
import numpy as np

plt.figure()
plt.plot(hist["tr_loss"], label="train")
plt.plot(hist["va_loss"], label="val")
plt.title("Loss per epoch"); plt.xlabel("epoch"); plt.ylabel("loss"); plt.legend(); plt.show()

plt.figure()
plt.plot(hist["tr_acc"], label="train")
plt.plot(hist["va_acc"], label="val")
plt.title("Accuracy per epoch"); plt.xlabel("epoch"); plt.ylabel("accuracy"); plt.legend(); plt.show()

plt.figure()
plt.plot(hist["tr_f1"], label="train")
plt.plot(hist["va_f1"], label="val")
plt.title("F1 (weighted) per epoch"); plt.xlabel("epoch"); plt.ylabel("F1"); plt.legend(); plt.show()

"""Part-8-Single video inference"""

if os.path.exists(MODEL_PATH):
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.to(DEVICE).eval()

def predict_video(video_path):
    y = read_audio_16k(video_path)
    A = wav2vec_feats(y)
    frames = iter_frames(video_path, (224,224))
    V = vis_feats_from_frames(frames)

    if 'WHISPERX_ACTIVE' in globals() and WHISPERX_ACTIVE and len(V)>0 and len(A)>0:
        try:
            import whisperx, cv2, numpy as np, torch
            audio_t = torch.tensor(y).float().to(DEVICE)
            with torch.no_grad():
                seg = asr_model.transcribe(audio_t, batch_size=8)
                ali = whisperx.align(seg["segments"], align_model, metadata, audio_t, 16000, DEVICE)
            dur = float(ali["word_segments"][-1]["end"]) if ali.get("word_segments") else len(y)/16000.0
            cap = cv2.VideoCapture(video_path); fps = cap.get(cv2.CAP_PROP_FPS) or 25.0; cap.release()
            t_v = np.arange(len(V), dtype=np.float32) / max(float(fps),1e-6); t_v = np.clip(t_v,0.0,max(dur,1e-6))
            Ta = len(A); idx = (t_v / max(dur,1e-6) * max(Ta-1,0)).round().astype(int); idx = np.clip(idx,0,max(Ta-1,0))
            A = A[idx]
        except Exception:
            A, V = linear_align(A, V)
    else:
        A, V = linear_align(A, V)

    X = np.concatenate([A, V], axis=1)[None, ...]
    L = np.array([X.shape[1]])
    with torch.no_grad():
        logits = model(torch.from_numpy(X).float().to(DEVICE), torch.from_numpy(L).to(DEVICE))
        p = logits.softmax(1).cpu().numpy()[0]
    lbl = "FAKE" if int(p.argmax())==1 else "REAL"
    return {"label": lbl, "prob_real": float(p[0]), "prob_fake": float(p[1])}

from google.colab import files
print("Upload a test videoâ€¦")
test_up = files.upload()
test_path = list(test_up.keys())[0]
print(predict_video(test_path))

"""Part-9-Results and ROC plots"""

import os, json, numpy as np, torch
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support, f1_score,
                             roc_auc_score, confusion_matrix, roc_curve)
import matplotlib.pyplot as plt
try:
    WORK_DIR
except NameError:
    WORK_DIR = "."

os.makedirs("figs", exist_ok=True)

if os.path.exists(MODEL_PATH):
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
model.to(DEVICE).eval()

def collect_outputs(loader):
    ys, ps = [], []
    with torch.no_grad():
        for X, Y, L in loader:
            X, Y, L = X.to(DEVICE), Y.to(DEVICE), L.to(DEVICE)
            prob = torch.softmax(model(X, L), dim=1).cpu().numpy()
            ys.extend(Y.cpu().numpy().tolist())
            ps.append(prob)
    return np.array(ys), (np.vstack(ps) if ps else np.zeros((0,2), np.float32))

y_true, prob = collect_outputs(val_loader)

if y_true.size == 0:
    print("No validation samples. Ensure preprocessing + split succeeded.")
else:
    counts = np.bincount(y_true, minlength=2)
    print(f"Validation class counts â†’ REAL={counts[0]}, FAKE={counts[1]}")

    y_pred = prob.argmax(1)
    acc = accuracy_score(y_true, y_pred)
    prec, rec, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted", zero_division=0
    )
    try:
        auc = roc_auc_score(y_true, prob[:, 1])
    except Exception:
        auc = float("nan")

    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])

    best = {"thr": 0.5, "f1": -1, "acc": 0, "prec": 0, "rec": 0}
    p_fake = prob[:, 1]
    for thr in np.linspace(0.25, 0.75, 21):
        y_hat = (p_fake >= thr).astype(int)
        f1v = f1_score(y_true, y_hat, average="weighted", zero_division=0)
        accv = accuracy_score(y_true, y_hat)
        precv, recv, _, _ = precision_recall_fscore_support(
            y_true, y_hat, average="weighted", zero_division=0
        )
        if f1v > best["f1"]:
            best = {"thr": float(thr), "f1": float(f1v),
                    "acc": float(accv), "prec": float(precv), "rec": float(recv)}

    print("=== Validation (argmax) ===")
    print(f"Accuracy: {acc:.4f} | F1(w): {f1:.4f} | Precision(w): {prec:.4f} | Recall(w): {rec:.4f} | ROC-AUC: {auc:.4f}")
    print("Confusion matrix [rows=true REAL,FAKE | cols=pred]:\n", cm)
    print("\n=== Tuned threshold ===")
    print(f"Best thr: {best['thr']:.2f} | Acc: {best['acc']:.4f} | F1(w): {best['f1']:.4f} | Precision: {best['prec']:.4f} | Recall: {best['rec']:.4f}")

    results = {
        "argmax": {
            "accuracy": float(acc),
            "f1_weighted": float(f1),
            "precision_weighted": float(prec),
            "recall_weighted": float(rec),
            "roc_auc": float(auc),
            "confusion_matrix": cm.tolist()
        },
        "tuned_threshold": best
    }
    with open(os.path.join(WORK_DIR, "results.json"), "w") as f:
        json.dump(results, f, indent=2)
    print("\nSaved:", os.path.join(WORK_DIR, "results.json"))

    plt.figure()
    im = plt.imshow(cm, interpolation="nearest")
    plt.title("Confusion Matrix (argmax)")
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.xticks([0, 1], ["REAL", "FAKE"]); plt.yticks([0, 1], ["REAL", "FAKE"])
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, str(cm[i, j]), ha="center", va="center")
    plt.colorbar(im)
    plt.tight_layout()
    plt.savefig("figs/confusion_matrix.png", dpi=300, bbox_inches="tight")
    plt.show()

    if np.unique(y_true).size == 2 and prob.shape[0] > 0:
        fpr, tpr, _ = roc_curve(y_true, prob[:, 1])
        plt.figure()
        plt.plot(fpr, tpr, label=f"ROC AUC={auc:.3f}")
        plt.plot([0, 1], [0, 1], '--')
        plt.title("ROC Curve (FAKE vs REAL)")
        plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
        plt.legend()
        plt.tight_layout()
        plt.savefig("figs/roc_curve.png", dpi=300, bbox_inches="tight")
        plt.show()
    else:
        print("ROC not plotted: validation set must contain BOTH classes (REAL and FAKE).")

"""Part-10-Mouth ROI Images"""

import cv2, numpy as np, matplotlib.pyplot as plt, os

def show_mouth_montage(video_path, n=8, out_path="figs/mouth_roi_montage.png"):
    frames = iter_frames(video_path, (224,224))
    if not frames:
        print("No frames."); return
    idx = np.linspace(0, len(frames)-1, min(n, len(frames))).round().astype(int)
    crops = [crop_mouth(frames[i]) for i in idx]
    strip = np.hstack(crops)
    plt.figure(figsize=(12,3))
    plt.imshow(strip)
    plt.axis("off"); plt.title("Mouth ROI montage")
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    plt.savefig(out_path, dpi=300, bbox_inches="tight"); plt.show()

real_dir = os.path.join(DATA_ROOT, "real")
cand = [os.path.join(real_dir,f) for f in os.listdir(real_dir) if f.lower().endswith((".mp4",".mov",".mkv",".avi",".webm"))]
if cand:
    show_mouth_montage(cand[0], n=8)
else:
    print("No real videos found for montage.")